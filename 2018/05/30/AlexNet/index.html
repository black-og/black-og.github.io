<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>AlexNet论文笔记 | 围城</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">AlexNet论文笔记</h1><a id="logo" href="/.">围城</a><p class="description">不忘初心</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 主页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">AlexNet论文笔记</h1><div class="post-meta">May 30, 2018<span> | </span><span class="category"><a href="/categories/深度学习/">深度学习</a></span></div><a class="disqus-comment-count" data-disqus-identifier="2018/05/30/AlexNet/" href="/2018/05/30/AlexNet/#disqus_thread"></a><div class="post-content"><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>AlexNet包含5个卷积层和3个全连接层，此外还包含一些max pooling层、LRN层(局部响应归一化)，具体结构如下：</p>
<ol>
<li>第一层为卷积层，共有96个大小为11*11*3的卷积核，步长为4。后跟LRN层和max pooling层。</li>
<li>第二层为卷积层，共有256个大小为5*5*96的卷积核，步长为1。后跟LRN层和max pooling层。</li>
<li>第三层为卷积层，共有384个大小为3*3*256的卷积核，步长为1。后面没有LRN层和max pooling层。</li>
<li>第四层为卷积层，共有384个大小为3*3*384的卷积核，步长为1。后面没有LRN层和max pooling层。</li>
<li>第五层为卷积层，共有256个大小为3*3*384的卷积核，步长为1。后面没有LRN层，只有max pooling层。</li>
<li>最后三层为全连接层，每一层都有4096个神经元。  </li>
</ol>
<p><strong>在所有层中，用的都是ReLU激活函数。并且上面提到的max pooling层的kernel都是3 * 3，步长为2(有重叠的pooling)</strong></p>
<h2 id="相关trick"><a href="#相关trick" class="headerlink" title="相关trick"></a>相关trick</h2><h3 id="一、加速trick"><a href="#一、加速trick" class="headerlink" title="一、加速trick"></a>一、加速trick</h3><h4 id="1-非饱和激活函数"><a href="#1-非饱和激活函数" class="headerlink" title="1. 非饱和激活函数"></a>1. 非饱和激活函数</h4><p>与之前常用的sigmoid和tanh等有饱和状态的函数不同，AlexNet中使用了此前Hiton等提出的ReLU(Rectified<br>Linear Units)函数，它是非饱和的，可以加速训练。此外，还有许多其他的优点。<a href="https://www.zybuluo.com/Blackog/note/987683" target="_blank" rel="noopener">(详细内容请戳)</a></p>
<h4 id="2-双GPU训练"><a href="#2-双GPU训练" class="headerlink" title="2. 双GPU训练"></a>2. 双GPU训练</h4><p>将网络放到两个GPU上训练，加速是次要的，主要还是因为当时单个的GPU很难能放得下AlexNet如此复杂的神经网络。作者使用的是两块显存为3GB的GTX 580，将网络参数分摊到两个GPU上并行训练。对于论文中给出的结构图，有些地方不是很理解，比如：<em>为什么有些卷积层之间只对上一层中和它在同一个GPU上的map进行卷积？不应该是对上一层所有的map进行卷积吗？</em></p>
<h3 id="二、防止过拟合trick"><a href="#二、防止过拟合trick" class="headerlink" title="二、防止过拟合trick"></a>二、防止过拟合trick</h3><h4 id="1-Dropout"><a href="#1-Dropout" class="headerlink" title="1. Dropout"></a>1. Dropout</h4><p>Dropout也是在AlexNet之前就由Hiton等人提出了，但是直到AlexNet才开始发扬光大。Dropout就是说，再训练的时候，所有的神经元都以一定的概率<em>p</em>被置为0，论文中取<em>p</em>=0.5，但是网络的权值都是共享的。这样，每个bacth送进网络进行训练的时候，相当于每次训练的网络结构都是不太相同的。最终训练完成之后，进行分类的时候，所有的神经元都不会被置为0，也就是说Dropout只发生在训练阶段。这样，最后的分类结果其实就相当于集成了多个不同的网络，效果自然会得到提升，泛化能力也强，在一定程度上可以减轻过拟合。  </p>
<p><strong>注意，在测试的时候，所有的神经元都要乘以_p_。个人理解应该是为了使得测试和训练的时候，这一层的输出在一个量级上，否则会影响下一层的输入。比如经过Dropout后，3*3的一个小邻域卷积后得到的结果为5，而没有Dropout时，可能这个结果是10，而后面的输入都是以5为输入进行训练的，所以当将10输入后，肯定会影响之后的结果。</strong></p>
<h4 id="2-数据增强"><a href="#2-数据增强" class="headerlink" title="2. 数据增强"></a>2. 数据增强</h4><p>也就是扩充数据集，因为AlexNet要学习的参数有很多，而样本集相对于这个巨大的参数量来说还是有些不够，这种情况下可能会导致过拟合，所以进行数据扩充也会缓解一下过拟合现象。AlexNet主要使用两种方式来对数据集进行扩充：<br>(1) 平移和旋转。步长为1，从256*256的图像中滑动提取224*224的新图像，这样每张图片得到的新图片为(256-224)^2=1024张。然后在进行水平翻转，最后每张图片可以得到1024*2=2048张图片。虽然说这样得到的图片其实都是极其相似的，但是也还是有效果的。<br>(2) <em>对RGB像素值进行降维，这一部分并不是很懂</em></p>
<h4 id="3-LRN"><a href="#3-LRN" class="headerlink" title="3. LRN"></a>3. LRN</h4><p>局部响应归一化，简单来说就是对于在<strong>同一位置</strong>，<strong>不同的map层</strong>之间的神经元进行归一化。使得值越大的神经元会更大，小的会更小。这样会增加泛化能力，毕竟这样的话，不同的输入对于这些神经元的影响也会变小，不会过于敏感。论文中还提到说这样会激发不同kernel得到的神经元之间的竞争。不过貌似提升不是很大，现在用的不多。</p>
<h4 id="4-Overlapping-Pooling"><a href="#4-Overlapping-Pooling" class="headerlink" title="4. Overlapping Pooling"></a>4. Overlapping Pooling</h4><p>顾名思义，就是在进行pooling操作时，会有重叠现象。举个例子，之前使用的pooling的kernel大小如果是3*3的话，一般步长也是3，那么上一次进行pooling操作的那些神经元，下一次就不会再出现了；而AlexNet使用的步长则是2，也就是说上一次进行pooling操作的那些神经元，下一次操作时还有一部分也会出现。这样得到的结果当然就更精细，可以一定程度上提升泛化能力。</p>
</div><div class="tags"><a href="/tags/ILSVRC-classification/">ILSVRC classification</a><a href="/tags/Deep-Learning/">Deep Learning</a></div><div class="post-nav"><a class="pre" href="/2018/05/30/VGGNet/">VGGNet论文笔记</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://black-og.github.io/2018/05/30/AlexNet/';
    this.page.identifier = '2018/05/30/AlexNet/';
    this.page.title = 'AlexNet论文笔记';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//black-og.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//black-og.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://black-og.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://black-og.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/ILSVRC-classification/" style="font-size: 15px;">ILSVRC classification</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/05/30/VGGNet/">VGGNet论文笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/30/AlexNet/">AlexNet论文笔记</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最新评论</i></div><script type="text/javascript" src="//black-og.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">围城.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>